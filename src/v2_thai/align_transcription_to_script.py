import difflib
import json
import os
from pprint import pprint

import numpy as np
from pythainlp.tokenize import word_tokenize

from src.v2_thai.Util_functions import save_json_file


def align_transcription_to_script_and_correct_timestamps(original_script, whisper_word_data, output_folder_path =""):
    """
    Aligns a perfect Thai script with messy, character-level Whisper timestamps.
    Returns a list of word-level segments with start/end times.
    """

    # 1. Tokenize the Perfect Script into Words
    # We use `pythainlp` here because Thai text is written without spaces (e.g., "ilovethai").
    # The `newmm` engine is a dictionary-based algorithm that knows how to split
    # "ilovethai" into ["i", "love", "thai"] correctly.
    tokenized_original_script_words = word_tokenize(original_script, engine="newmm", keep_whitespace=False)

    # Filter out empty strings just in case the tokenizer produced blank items (e.g., from double spaces).
    tokenized_original_script_words = [w for w in tokenized_original_script_words if w.strip()]

    # 2. Flatten Whisper Data for Alignment
    # Whisper returns data in chunks, sometimes grouping multiple characters like "cat" into one timestamp.
    # To compare strictly, we need to flatten this into a character-by-character stream.
    # We create two parallel lists:
    # `whisper_chars_string`: A single string of everything the AI heard (e.g., "c" "a" "t").
    # `whisper_string_time_map`: A list where index N holds the timestamp for the character at index N.
    # e.g. "catsarecute", index=3 will give the start and end timestamps of `s`
    whisper_chars_string = ""
    whisper_string_time_map = [] # stores index mapping to (start, end).
    # e.g. will look like [{'start'=0, 'end'=0.5}, {'start'=0.6, 'end'=1.1}, ...]

    for item in whisper_word_data:
        word_fragment_char = item['word']
        start = item['start']
        end = item['end']

        # Whisper might output multiple chars in one block, or single chars.
        # We loop through them to assign the same timestamp to every character in that block. (thus the nested for-loop)
        # if it is just a char then it just do one loop.
        # Example: if Whisper says "Hi" is at 1.0s, then 'H' is at 1.0s and 'i' is at 1.0s.
        for char in word_fragment_char:
            whisper_chars_string += char
            whisper_string_time_map.append({'start': start, 'end': end})


    # 3. Create a Comparison String from the Script
    # We join our "Perfect Script" words back into one long string.
    # Now we have two massive strings to compare:
    # 1. `full_original_script_string`: The correct text (Reference).
    # 2. `whisper_chars_string`: The messy audio text (Hypothesis).
    full_original_script_string = "".join(tokenized_original_script_words)

    # 4. Use SequenceMatcher to find the best alignment
    # `difflib.SequenceMatcher` is the core engine here. It solves the "Longest Common Subsequence" problem.
    # It compares the two strings and figures out how to align them, even if Whisper had typos
    # (e.g., matching "Hello" in script to "Hullo" in audio).
    matcher = difflib.SequenceMatcher(None, full_original_script_string, whisper_chars_string)

    final_aligned_words = []
    original_script_char_index = 0

    # We initialize a cursor to remember the last matched character index in the audio.
    last_audio_search_index = 0

    # Iterate through our original perfect script words one by one.
    # We need to find where each specific word lives inside the audio timeline.
    for word in tokenized_original_script_words:
        word_len = len(word)

        # Calculate exactly which characters in `full_original_script_string` belong to this word.
        # Example: If the first word is 2 chars long, we look at index 0 to 2.
        word_start_char_index = original_script_char_index
        word_end_char_index = original_script_char_index + word_len

        # Get the opcodes (match instructions) for this specific word's range.
        # `find_longest_match` asks the matcher:
        # "Look at the audio string (... to len). Where is the best match for THIS specific script word?"
        # Instead of searching from 0, we search from 'last_audio_search_index'.
        # This prevents the code from looking backwards in time.
        matches = matcher.find_longest_match(
            word_start_char_index, word_end_char_index,
            last_audio_search_index, len(whisper_chars_string)
        )

        # =========================================================
        # FALLBACK / DEFAULT TIME LOGIC
        # =========================================================
        # Sometimes, the audio is so bad or the script is so different that `matcher` finds NO match.
        # We cannot set the time to 0.0, or the subtitles would flash at the start of the video.
        # Instead, we guess: "This word probably happens right after the previous word."
        start_time = 0.0
        end_time = 0.0

        # now that the start and end times are known, we continue...
        if final_aligned_words:
            # Grab the end time of the word we just finished processing.
            start_time = final_aligned_words[-1]['end']

            # We add a tiny arbitrary duration (0.1s) just so the word exists on the timeline.
            # This ensures the subtitles flow continuously without crashing or resetting to zero.
            end_time = start_time + 0.1

        # =========================================================
        # MATCH FOUND LOGIC
        # =========================================================
        # If `matches.size > 0`, it means `difflib` found this word (or part of it) in the audio text.
        if matches.size > 0:
            # `matches.b` is the starting index in the `whisper_chars_string` (the audio text).
            # `matches.size` is how many characters matched.

            whisper_start_idx = matches.b
            whisper_end_idx = matches.b + matches.size - 1

            # We use these indices to look up the actual floating-point seconds
            # from the map we built in Step 2.

            # Safety check: ensure we don't look past the end of the list.
            if whisper_start_idx < len(whisper_string_time_map):
                start_time = whisper_string_time_map[whisper_start_idx]['start']

            if whisper_end_idx < len(whisper_string_time_map):
                end_time = whisper_string_time_map[whisper_end_idx]['end']

            # HEURISTIC FIX NOTE:
            # Sometimes a script word is 5 chars, but Whisper only pronounced 3 of them clearly.
            # `difflib` might say "I only matched 3 chars".
            # Ideally, we might want to extend the end_time to fill the gap, but the current
            # implementation keeps it simple and strictly uses the matched time. (idk how correct or outdated this comment is)


            # Move our search cursor to the end of this match.
            # The next word MUST be found after this point.
            last_audio_search_index = matches.b + matches.size

        # Append the Correct Word with the found (or fallback) Audio Timestamp
        final_aligned_words.append({
            "word": word,
            "start": float(start_time),
            "end": float(end_time)
        })

        # Advance the index tracker so the next loop knows where the next word begins
        original_script_char_index += word_len

    # 5. Post-Processing Cleanup (Fill gaps)
    # Because `difflib` is fuzzy, and audio data is messy, we might end up with illogical times.
    # Example: Word A ends at 5.0s, but Word B starts at 4.8s. This overlap looks bad on screen.
    for i in range(len(final_aligned_words) - 1):
        current_word = final_aligned_words[i]
        next_word = final_aligned_words[i+1]

        # Check for overlap: If the current word runs past the start of the next word.
        if current_word['end'] > next_word['start']:
            # We fix it by cutting the current word short.
            # We set the current word's end time exactly to the next word's start time.
            # This creates a seamless "karaoke" transition with no overlaps.
            current_word['end'] = next_word['start']


        # we also want to keep the decimals max 2 to avoid floating point trailing numbers (e.g. 34.74000000000001)
        current_word['start'] = round(current_word['start'], 2)
        current_word['end'] = round(current_word['end'], 2)

    # --- 6? save the data as a json file for inspection
    output_json_file_name = "debug_deletelater_aligned_and_corrected_transcription.json"
    full_json_save_path = os.path.join(output_folder_path, output_json_file_name)
    save_json_file(final_aligned_words, full_json_save_path)

    return final_aligned_words

# ================== EXECUTION

if __name__ == "__main__":
    test_perfect_thai_script = "แกรรร! เรื่องนี้คือพีคในพีค! เมื่อคืนแฟนฉันโทรมาเสียงสั่นๆ บอกว่า 'ที่รัก...เราเจอพี่สาวเธอ' ฉันก็แบบ อ๋อเหรอ เจอที่ไหน สยามป้ะ? แต่สิ่งที่ฮีตอบกลับมาคือ... 'เจอในอาบอบนวด!!!' คือฉันช็อคไปสามวิ! ห๊ะ!? พี่สาวฉันเนี่ยนะ!? ผู้หญิงที่เรียบร้อยดุจผ้าพับไว้ ใส่บาตรทุกเช้าเนี่ยนะ!? เป็นไปไม่ได้! ฉันก็ถามย้ำว่าแน่ใจนะ เห็นผิดรึเปล่า? แฟนก็ยืนยันว่าเห็นแว๊บๆ เดินหายเข้าไปหลังร้าน ในหัวฉันตอนนั้นคือตีกันไปหมด บ้านเรามีปัญหาการเงินเหรอ? หรือพี่มีด้านมืดที่เราไม่รู้? คือดราม่ามาก! ฉันทนไม่ไหว ตัดสินใจโทรหาพี่สาวทันที! พอพี่รับปุ๊บ ฉันใส่เลย 'แกไปทำอะไรที่อาบอบนวด!' พี่ฉันก็เงียบไปแป๊บนึง แล้วตอบกลับมาว่า... 'อ๋ออ พอดีป้าแม่บ้านเค้าลา พี่เลยมารับจ๊อบขัดห้องน้ำแทน ได้ตั้งวันละ 500 แหนะ' ...แม่เจ้าโว้ยยยย! โล่งอกไปที! สรุปนะ... ที่พีคกว่าเรื่องพี่สาวฉันไปขัดห้องน้ำ ก็คือคำถามที่ว่า... แล้วแฟนฉันไปทำอะไรที่นั่นก่อนนนนน!?"

    sample_raw_whisper_word_data = [{'word': 'แ', 'start': np.float64(0.0), 'end': np.float64(0.32)}, {'word': 'ก', 'start': np.float64(0.32), 'end': np.float64(0.36)}, {'word': 'ร', 'start': np.float64(0.36), 'end': np.float64(0.7)}, {'word': 'า', 'start': np.float64(0.7), 'end': np.float64(0.88)}, {'word': 'เร', 'start': np.float64(1.04), 'end': np.float64(1.3)}, {'word': 'ื่', 'start': np.float64(1.3), 'end': np.float64(1.3)}, {'word': 'อง', 'start': np.float64(1.3), 'end': np.float64(1.36)}, {'word': 'น', 'start': np.float64(1.36), 'end': np.float64(1.48)}, {'word': 'ี้', 'start': np.float64(1.48), 'end': np.float64(1.52)}, {'word': 'ค', 'start': np.float64(1.52), 'end': np.float64(1.66)}, {'word': 'ื', 'start': np.float64(1.66), 'end': np.float64(1.66)}, {'word': 'อ', 'start': np.float64(1.66), 'end': np.float64(1.74)}, {'word': 'พ', 'start': np.float64(1.74), 'end': np.float64(1.94)}, {'word': 'ี', 'start': np.float64(1.94), 'end': np.float64(2.0)}, {'word': 'ก', 'start': np.float64(2.0), 'end': np.float64(2.1)}, {'word': 'ใ', 'start': np.float64(2.1), 'end': np.float64(2.22)}, {'word': 'น', 'start': np.float64(2.22), 'end': np.float64(2.26)}, {'word': 'พ', 'start': np.float64(2.26), 'end': np.float64(2.48)}, {'word': 'ี', 'start': np.float64(2.48), 'end': np.float64(2.58)}, {'word': 'ก', 'start': np.float64(2.58), 'end': np.float64(2.72)}, {'word': 'เม', 'start': np.float64(2.72), 'end': np.float64(3.1)}, {'word': 'ื่', 'start': np.float64(3.1), 'end': np.float64(3.12)}, {'word': 'อ', 'start': np.float64(3.12), 'end': np.float64(3.16)}, {'word': 'ค', 'start': np.float64(3.16), 'end': np.float64(3.28)}, {'word': 'ื', 'start': np.float64(3.28), 'end': np.float64(3.32)}, {'word': 'น', 'start': np.float64(3.32), 'end': np.float64(3.4)}, {'word': 'แ', 'start': np.float64(3.4), 'end': np.float64(3.6)}, {'word': 'ฟ', 'start': np.float64(3.6), 'end': np.float64(3.6)}, {'word': 'น', 'start': np.float64(3.6), 'end': np.float64(3.7)}, {'word': 'ฉ', 'start': np.float64(3.7), 'end': np.float64(3.86)}, {'word': 'ั', 'start': np.float64(3.86), 'end': np.float64(3.9)}, {'word': 'น', 'start': np.float64(3.9), 'end': np.float64(3.94)}, {'word': 'โ', 'start': np.float64(3.94), 'end': np.float64(4.08)}, {'word': 'ท', 'start': np.float64(4.08), 'end': np.float64(4.12)}, {'word': 'ร', 'start': np.float64(4.12), 'end': np.float64(4.2)}, {'word': 'มา', 'start': np.float64(4.2), 'end': np.float64(4.28)}, {'word': 'เส', 'start': np.float64(4.28), 'end': np.float64(4.46)}, {'word': 'ี', 'start': np.float64(4.46), 'end': np.float64(4.5)}, {'word': 'ย', 'start': np.float64(4.5), 'end': np.float64(4.52)}, {'word': 'ง', 'start': np.float64(4.52), 'end': np.float64(4.6)}, {'word': 'ส', 'start': np.float64(4.6), 'end': np.float64(4.74)}, {'word': 'ั่', 'start': np.float64(4.74), 'end': np.float64(4.76)}, {'word': 'น', 'start': np.float64(4.76), 'end': np.float64(4.82)}, {'word': 'ๆ', 'start': np.float64(4.82), 'end': np.float64(5.1)}, {'word': 'บ', 'start': np.float64(5.26), 'end': np.float64(5.52)}, {'word': 'อก', 'start': np.float64(5.52), 'end': np.float64(5.62)}, {'word': 'ว', 'start': np.float64(5.62), 'end': np.float64(5.8)}, {'word': '่', 'start': np.float64(5.8), 'end': np.float64(5.88)}, {'word': 'า', 'start': np.float64(5.88), 'end': np.float64(5.94)}, {'word': 'ท', 'start': np.float64(6.16), 'end': np.float64(6.36)}, {'word': 'ี่', 'start': np.float64(6.36), 'end': np.float64(6.48)}, {'word': 'ร', 'start': np.float64(6.48), 'end': np.float64(6.64)}, {'word': 'ั', 'start': np.float64(6.64), 'end': np.float64(6.7)}, {'word': 'ก', 'start': np.float64(6.7), 'end': np.float64(6.76)}, {'word': 'เรา', 'start': np.float64(6.9), 'end': np.float64(7.12)}, {'word': 'เจ', 'start': np.float64(7.12), 'end': np.float64(7.32)}, {'word': 'อ', 'start': np.float64(7.32), 'end': np.float64(7.38)}, {'word': 'พ', 'start': np.float64(7.38), 'end': np.float64(7.52)}, {'word': 'ี่', 'start': np.float64(7.52), 'end': np.float64(7.58)}, {'word': 'ส', 'start': np.float64(7.58), 'end': np.float64(7.7)}, {'word': 'า', 'start': np.float64(7.7), 'end': np.float64(7.78)}, {'word': 'ว', 'start': np.float64(7.78), 'end': np.float64(7.86)}, {'word': 'เธ', 'start': np.float64(7.86), 'end': np.float64(8.02)}, {'word': 'อ', 'start': np.float64(8.02), 'end': np.float64(8.26)}, {'word': 'ฉ', 'start': np.float64(8.52), 'end': np.float64(8.72)}, {'word': 'ั', 'start': np.float64(8.72), 'end': np.float64(8.78)}, {'word': 'น', 'start': np.float64(8.78), 'end': np.float64(8.82)}, {'word': 'ก', 'start': np.float64(8.82), 'end': np.float64(8.94)}, {'word': '็', 'start': np.float64(8.94), 'end': np.float64(8.94)}, {'word': 'แ', 'start': np.float64(8.94), 'end': np.float64(9.04)}, {'word': 'บ', 'start': np.float64(9.04), 'end': np.float64(9.06)}, {'word': 'บ', 'start': np.float64(9.06), 'end': np.float64(9.2)}, {'word': 'อ', 'start': np.float64(9.54), 'end': np.float64(9.64)}, {'word': '๋', 'start': np.float64(9.64), 'end': np.float64(9.72)}, {'word': 'อ', 'start': np.float64(9.72), 'end': np.float64(9.78)}, {'word': 'ห', 'start': np.float64(9.78), 'end': np.float64(9.86)}, {'word': 'ร', 'start': np.float64(9.86), 'end': np.float64(9.92)}, {'word': 'อ', 'start': np.float64(9.92), 'end': np.float64(10.04)}, {'word': 'เจ', 'start': np.float64(10.04), 'end': np.float64(10.46)}, {'word': 'อ', 'start': np.float64(10.46), 'end': np.float64(10.54)}, {'word': 'ท', 'start': np.float64(10.54), 'end': np.float64(10.64)}, {'word': 'ี่', 'start': np.float64(10.64), 'end': np.float64(10.72)}, {'word': 'ไ', 'start': np.float64(10.72), 'end': np.float64(10.78)}, {'word': 'หน', 'start': np.float64(10.78), 'end': np.float64(10.96)}, {'word': 'ส', 'start': np.float64(11.3), 'end': np.float64(11.4)}, {'word': 'ย', 'start': np.float64(11.4), 'end': np.float64(11.54)}, {'word': 'าม', 'start': np.float64(11.54), 'end': np.float64(11.7)}, {'word': 'ป', 'start': np.float64(11.7), 'end': np.float64(11.86)}, {'word': '่', 'start': np.float64(11.86), 'end': np.float64(11.9)}, {'word': 'ะ', 'start': np.float64(11.9), 'end': np.float64(11.98)}, {'word': 'แต', 'start': np.float64(12.280000000000001), 'end': np.float64(12.48)}, {'word': '่', 'start': np.float64(12.48), 'end': np.float64(12.6)}, {'word': 'ส', 'start': np.float64(12.6), 'end': np.float64(12.72)}, {'word': 'ิ', 'start': np.float64(12.72), 'end': np.float64(12.72)}, {'word': '่', 'start': np.float64(12.72), 'end': np.float64(12.74)}, {'word': 'ง', 'start': np.float64(12.74), 'end': np.float64(12.78)}, {'word': 'ท', 'start': np.float64(12.78), 'end': np.float64(12.88)}, {'word': 'ี่', 'start': np.float64(12.88), 'end': np.float64(12.92)}, {'word': 'ฮ', 'start': np.float64(12.92), 'end': np.float64(13.06)}, {'word': 'ี', 'start': np.float64(13.06), 'end': np.float64(13.16)}, {'word': 'ต', 'start': np.float64(13.16), 'end': np.float64(13.3)}, {'word': 'อ', 'start': np.float64(13.3), 'end': np.float64(13.34)}, {'word': 'บ', 'start': np.float64(13.34), 'end': np.float64(13.42)}, {'word': 'ก', 'start': np.float64(13.42), 'end': np.float64(13.56)}, {'word': 'ล', 'start': np.float64(13.56), 'end': np.float64(13.58)}, {'word': 'ั', 'start': np.float64(13.58), 'end': np.float64(13.58)}, {'word': 'บ', 'start': np.float64(13.58), 'end': np.float64(13.64)}, {'word': 'มา', 'start': np.float64(13.64), 'end': np.float64(13.78)}, {'word': 'ค', 'start': np.float64(13.78), 'end': np.float64(13.98)}, {'word': 'ื', 'start': np.float64(13.98), 'end': np.float64(14.06)}, {'word': 'อ', 'start': np.float64(14.06), 'end': np.float64(14.16)}, {'word': 'เจ', 'start': np.float64(14.48), 'end': np.float64(14.68)}, {'word': 'อ', 'start': np.float64(14.68), 'end': np.float64(14.78)}, {'word': 'ใ', 'start': np.float64(14.78), 'end': np.float64(14.88)}, {'word': 'น', 'start': np.float64(14.88), 'end': np.float64(14.94)}, {'word': 'อ', 'start': np.float64(14.94), 'end': np.float64(15.06)}, {'word': 'า', 'start': np.float64(15.06), 'end': np.float64(15.12)}, {'word': 'บ', 'start': np.float64(15.12), 'end': np.float64(15.2)}, {'word': 'อ', 'start': np.float64(15.2), 'end': np.float64(15.32)}, {'word': 'อ', 'start': np.float64(15.32), 'end': np.float64(15.34)}, {'word': 'บ', 'start': np.float64(15.34), 'end': np.float64(15.42)}, {'word': 'น', 'start': np.float64(15.42), 'end': np.float64(15.62)}, {'word': 'ว', 'start': np.float64(15.62), 'end': np.float64(15.78)}, {'word': 'ด', 'start': np.float64(15.78), 'end': np.float64(15.94)}, {'word': 'ค', 'start': np.float64(15.94), 'end': np.float64(16.34)}, {'word': 'ื', 'start': np.float64(16.34), 'end': np.float64(16.36)}, {'word': 'อ', 'start': np.float64(16.36), 'end': np.float64(16.38)}, {'word': 'ฉ', 'start': np.float64(16.38), 'end': np.float64(16.48)}, {'word': 'ั', 'start': np.float64(16.48), 'end': np.float64(16.58)}, {'word': 'น', 'start': np.float64(16.58), 'end': np.float64(16.58)}, {'word': 'ช', 'start': np.float64(16.58), 'end': np.float64(16.76)}, {'word': '็', 'start': np.float64(16.76), 'end': np.float64(16.8)}, {'word': 'อ', 'start': np.float64(16.8), 'end': np.float64(16.86)}, {'word': 'ค', 'start': np.float64(16.86), 'end': np.float64(16.86)}, {'word': 'ไป', 'start': np.float64(16.86), 'end': np.float64(16.98)}, {'word': 'ส', 'start': np.float64(16.98), 'end': np.float64(17.2)}, {'word': 'าม', 'start': np.float64(17.2), 'end': np.float64(17.36)}, {'word': 'ว', 'start': np.float64(17.36), 'end': np.float64(17.56)}, {'word': 'ิ', 'start': np.float64(17.56), 'end': np.float64(17.6)}, {'word': 'ต', 'start': np.float64(17.6), 'end': np.float64(17.7)}, {'word': 'ฮ', 'start': np.float64(17.900000000000002), 'end': np.float64(18.1)}, {'word': 'ะ', 'start': np.float64(18.1), 'end': np.float64(18.22)}, {'word': 'พ', 'start': np.float64(18.56), 'end': np.float64(18.66)}, {'word': 'ี่', 'start': np.float64(18.66), 'end': np.float64(18.72)}, {'word': 'ส', 'start': np.float64(18.72), 'end': np.float64(18.84)}, {'word': 'า', 'start': np.float64(18.84), 'end': np.float64(18.9)}, {'word': 'ว', 'start': np.float64(18.9), 'end': np.float64(18.98)}, {'word': 'ฉ', 'start': np.float64(18.98), 'end': np.float64(19.1)}, {'word': 'ั', 'start': np.float64(19.1), 'end': np.float64(19.2)}, {'word': 'นน', 'start': np.float64(19.2), 'end': np.float64(19.28)}, {'word': 'ี่', 'start': np.float64(19.28), 'end': np.float64(19.4)}, {'word': 'นะ', 'start': np.float64(19.4), 'end': np.float64(19.54)}, {'word': 'ผ', 'start': np.float64(19.82), 'end': np.float64(20.02)}, {'word': 'ู้', 'start': np.float64(20.02), 'end': np.float64(20.1)}, {'word': 'ห', 'start': np.float64(20.1), 'end': np.float64(20.18)}, {'word': 'ญ', 'start': np.float64(20.18), 'end': np.float64(20.18)}, {'word': 'ิ', 'start': np.float64(20.18), 'end': np.float64(20.22)}, {'word': 'ง', 'start': np.float64(20.22), 'end': np.float64(20.26)}, {'word': 'ท', 'start': np.float64(20.26), 'end': np.float64(20.36)}, {'word': 'ี่', 'start': np.float64(20.36), 'end': np.float64(20.44)}, {'word': 'เร', 'start': np.float64(20.44), 'end': np.float64(20.58)}, {'word': 'ี', 'start': np.float64(20.58), 'end': np.float64(20.6)}, {'word': 'ย', 'start': np.float64(20.6), 'end': np.float64(20.66)}, {'word': 'บ', 'start': np.float64(20.66), 'end': np.float64(20.74)}, {'word': 'ร', 'start': np.float64(20.74), 'end': np.float64(20.92)}, {'word': '้', 'start': np.float64(20.92), 'end': np.float64(20.98)}, {'word': 'อย', 'start': np.float64(20.98), 'end': np.float64(21.02)}, {'word': 'ด', 'start': np.float64(21.02), 'end': np.float64(21.14)}, {'word': 'ุ', 'start': np.float64(21.14), 'end': np.float64(21.22)}, {'word': 'ด', 'start': np.float64(21.22), 'end': np.float64(21.22)}, {'word': 'ภ', 'start': np.float64(21.22), 'end': np.float64(21.38)}, {'word': 'า', 'start': np.float64(21.38), 'end': np.float64(21.44)}, {'word': 'พ', 'start': np.float64(21.44), 'end': np.float64(21.52)}, {'word': 'พ', 'start': np.float64(21.52), 'end': np.float64(21.64)}, {'word': 'บ', 'start': np.float64(21.64), 'end': np.float64(21.7)}, {'word': 'ไ', 'start': np.float64(21.7), 'end': np.float64(21.96)}, {'word': 'ว', 'start': np.float64(21.96), 'end': np.float64(21.96)}, {'word': '้', 'start': np.float64(21.96), 'end': 22.2}, {'word': 'ใ', 'start': np.float64(22.2), 'end': np.float64(22.52)}, {'word': 'ส', 'start': np.float64(22.52), 'end': np.float64(22.54)}, {'word': '่', 'start': np.float64(22.54), 'end': np.float64(22.62)}, {'word': 'บ', 'start': np.float64(22.62), 'end': np.float64(22.76)}, {'word': 'า', 'start': np.float64(22.76), 'end': np.float64(22.84)}, {'word': 'ท', 'start': np.float64(22.84), 'end': np.float64(22.84)}, {'word': 'ท', 'start': np.float64(22.84), 'end': np.float64(22.98)}, {'word': 'ุ', 'start': np.float64(22.98), 'end': np.float64(23.0)}, {'word': 'ก', 'start': np.float64(23.0), 'end': np.float64(23.04)}, {'word': 'เช', 'start': np.float64(23.04), 'end': np.float64(23.2)}, {'word': '้', 'start': np.float64(23.2), 'end': np.float64(23.32)}, {'word': 'าน', 'start': np.float64(23.32), 'end': np.float64(23.5)}, {'word': 'ี่', 'start': np.float64(23.5), 'end': np.float64(23.5)}, {'word': 'นะ', 'start': np.float64(23.5), 'end': np.float64(23.68)}, {'word': 'เป', 'start': np.float64(23.88), 'end': np.float64(24.22)}, {'word': '็', 'start': np.float64(24.22), 'end': np.float64(24.24)}, {'word': 'น', 'start': np.float64(24.24), 'end': np.float64(24.3)}, {'word': 'ไป', 'start': np.float64(24.3), 'end': np.float64(24.44)}, {'word': 'ไม', 'start': np.float64(24.44), 'end': np.float64(24.6)}, {'word': '่', 'start': np.float64(24.6), 'end': np.float64(24.66)}, {'word': 'ได', 'start': np.float64(24.66), 'end': np.float64(24.76)}, {'word': '้', 'start': np.float64(24.76), 'end': np.float64(24.98)}, {'word': 'ฉ', 'start': np.float64(24.98), 'end': np.float64(25.38)}, {'word': 'ั', 'start': np.float64(25.38), 'end': np.float64(25.42)}, {'word': 'น', 'start': np.float64(25.42), 'end': np.float64(25.48)}, {'word': 'ก', 'start': np.float64(25.48), 'end': np.float64(25.58)}, {'word': '็', 'start': np.float64(25.58), 'end': np.float64(25.6)}, {'word': 'ท', 'start': np.float64(25.6), 'end': np.float64(25.74)}, {'word': '�', 'start': np.float64(25.74), 'end': np.float64(25.82)}, {'word': '�', 'start': np.float64(25.82), 'end': np.float64(25.82)}, {'word': '�', 'start': np.float64(25.82), 'end': np.float64(25.82)}, {'word': '�', 'start': np.float64(25.82), 'end': np.float64(25.82)}, {'word': 'ұ', 'start': np.float64(25.82), 'end': np.float64(25.82)}, {'word': 'ม', 'start': np.float64(25.82), 'end': np.float64(25.88)}, {'word': 'ย', 'start': np.float64(25.88), 'end': np.float64(26.02)}, {'word': '้', 'start': np.float64(26.02), 'end': np.float64(26.06)}, {'word': 'ำ', 'start': np.float64(26.06), 'end': np.float64(26.12)}, {'word': 'ว', 'start': np.float64(26.12), 'end': np.float64(26.3)}, {'word': '่', 'start': np.float64(26.3), 'end': np.float64(26.36)}, {'word': 'า', 'start': np.float64(26.36), 'end': np.float64(26.42)}, {'word': 'แ', 'start': np.float64(26.72), 'end': np.float64(26.8)}, {'word': 'น', 'start': np.float64(26.8), 'end': np.float64(26.82)}, {'word': '่', 'start': np.float64(26.82), 'end': np.float64(26.92)}, {'word': 'ใ', 'start': np.float64(26.92), 'end': np.float64(27.04)}, {'word': 'จ', 'start': np.float64(27.04), 'end': np.float64(27.14)}, {'word': 'นะ!', 'start': np.float64(27.14), 'end': np.float64(27.34)}, {'word': 'เห', 'start': np.float64(27.54), 'end': np.float64(27.74)}, {'word': '็', 'start': np.float64(27.74), 'end': np.float64(27.76)}, {'word': 'น', 'start': np.float64(27.76), 'end': np.float64(27.82)}, {'word': 'ผ', 'start': np.float64(27.82), 'end': np.float64(27.96)}, {'word': 'ิ', 'start': np.float64(27.96), 'end': np.float64(28.06)}, {'word': 'ด', 'start': np.float64(28.06), 'end': np.float64(28.06)}, {'word': 'ท', 'start': np.float64(28.06), 'end': np.float64(28.14)}, {'word': 'ұ', 'start': np.float64(28.14), 'end': np.float64(28.14)}, {'word': 'ล', 'start': np.float64(28.14), 'end': np.float64(28.26)}, {'word': 'ู', 'start': np.float64(28.26), 'end': np.float64(28.26)}, {'word': 'ก!', 'start': np.float64(28.26), 'end': np.float64(28.26)}, {'word': 'แ', 'start': np.float64(28.740000000000002), 'end': np.float64(28.96)}, {'word': 'ท', 'start': np.float64(28.96), 'end': np.float64(29.04)}, {'word': 'น', 'start': np.float64(29.04), 'end': np.float64(29.12)}, {'word': 'ก', 'start': np.float64(29.12), 'end': np.float64(29.22)}, {'word': '็', 'start': np.float64(29.22), 'end': np.float64(29.28)}, {'word': 'ย', 'start': np.float64(29.28), 'end': np.float64(29.4)}, {'word': '�', 'start': np.float64(29.4), 'end': np.float64(29.44)}, {'word': '�', 'start': np.float64(29.44), 'end': np.float64(29.44)}, {'word': '�', 'start': np.float64(29.44), 'end': np.float64(29.44)}, {'word': '�', 'start': np.float64(29.44), 'end': np.float64(29.44)}, {'word': '�', 'start': np.float64(29.44), 'end': np.float64(29.44)}, {'word': 'ұ', 'start': np.float64(29.44), 'end': np.float64(29.44)}, {'word': 'ұ', 'start': np.float64(29.44), 'end': np.float64(29.46)}, {'word': 'ұ', 'start': np.float64(29.46), 'end': np.float64(29.7)}, {'word': 'ұ', 'start': np.float64(29.7), 'end': np.float64(30.32)}, {'word': 'ұ', 'start': np.float64(30.32), 'end': np.float64(30.34)}, {'word': 'ұ', 'start': np.float64(30.34), 'end': np.float64(30.42)}, {'word': 'ұ', 'start': np.float64(30.42), 'end': np.float64(38.42)}, {'word': 'ұ', 'start': np.float64(38.42), 'end': np.float64(43.0)}, {'word': 'ұ', 'start': np.float64(43.0), 'end': np.float64(43.04)}, {'word': 'ұ', 'start': np.float64(43.04), 'end': np.float64(43.04)}, {'word': 'ұ', 'start': np.float64(43.04), 'end': np.float64(43.04)}, {'word': 'ұ', 'start': np.float64(43.04), 'end': np.float64(43.04)}, {'word': 'ұ', 'start': np.float64(43.04), 'end': np.float64(43.04)}, {'word': 'ұ', 'start': np.float64(43.04), 'end': np.float64(43.04)}, {'word': 'ұ', 'start': np.float64(43.04), 'end': np.float64(43.12)}, {'word': 'ұ', 'start': np.float64(43.12), 'end': np.float64(43.14)}, {'word': 'ұ', 'start': np.float64(43.14), 'end': np.float64(43.22)}, {'word': 'ұ', 'start': np.float64(43.22), 'end': np.float64(43.24)}, {'word': 'ұ', 'start': np.float64(43.24), 'end': np.float64(43.3)}, {'word': 'ұ', 'start': np.float64(43.3), 'end': np.float64(44.08)}, {'word': 'ұ', 'start': np.float64(44.08), 'end': np.float64(51.34)}, {'word': 'ұ', 'start': np.float64(51.34), 'end': np.float64(55.88)}, {'word': 'ұ', 'start': np.float64(55.88), 'end': np.float64(56.04)}, {'word': 'ұ', 'start': np.float64(56.04), 'end': np.float64(56.16)}, {'word': 'ұ', 'start': np.float64(56.16), 'end': np.float64(56.42)}, {'word': 'ұ', 'start': np.float64(56.42), 'end': np.float64(57.42)}, {'word': 'ұ', 'start': np.float64(57.42), 'end': np.float64(57.64)}, {'word': 'ұ', 'start': np.float64(57.64), 'end': np.float64(57.64)}, {'word': 'ұ', 'start': np.float64(57.64), 'end': np.float64(57.64)}, {'word': 'ұ', 'start': np.float64(57.64), 'end': np.float64(57.64)}, {'word': 'ұ', 'start': np.float64(57.64), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.68)}, {'word': 'ұ', 'start': np.float64(57.68), 'end': np.float64(57.72)}, {'word': 'ұ', 'start': np.float64(57.72), 'end': np.float64(57.92)}, {'word': 'ұ', 'start': np.float64(57.92), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'ұ', 'start': np.float64(58.14), 'end': np.float64(58.14)}, {'word': 'แ', 'start': np.float64(58.14), 'end': np.float64(58.48)}, {'word': 'ม', 'start': np.float64(58.48), 'end': np.float64(58.56)}, {'word': '่', 'start': np.float64(58.56), 'end': np.float64(58.78)}, {'word': 'เจ', 'start': np.float64(58.78), 'end': np.float64(59.0)}, {'word': '้', 'start': np.float64(59.0), 'end': np.float64(59.04)}, {'word': 'า', 'start': np.float64(59.04), 'end': np.float64(59.2)}, {'word': 'โ', 'start': np.float64(59.2), 'end': np.float64(59.32)}, {'word': 'ว', 'start': np.float64(59.32), 'end': np.float64(59.44)}, {'word': '้', 'start': np.float64(59.44), 'end': np.float64(59.58)}, {'word': 'ย', 'start': np.float64(59.58), 'end': np.float64(59.64)}, {'word': 'ล', 'start': np.float64(60.04), 'end': np.float64(60.18)}, {'word': 'ง', 'start': np.float64(60.18), 'end': np.float64(60.32)}, {'word': 'อ', 'start': np.float64(60.32), 'end': np.float64(60.54)}, {'word': 'อก', 'start': np.float64(60.54), 'end': np.float64(60.66)}, {'word': 'ไป', 'start': np.float64(60.66), 'end': np.float64(60.8)}, {'word': 'ท', 'start': np.float64(60.8), 'end': np.float64(61.0)}, {'word': 'ี', 'start': np.float64(61.0), 'end': np.float64(61.22)}, {'word': 'ส', 'start': np.float64(61.5), 'end': np.float64(61.74)}, {'word': 'ร', 'start': np.float64(61.74), 'end': np.float64(61.88)}, {'word': 'ุ', 'start': np.float64(61.88), 'end': np.float64(61.92)}, {'word': 'ด', 'start': np.float64(61.92), 'end': np.float64(61.98)}, {'word': 'นะ', 'start': np.float64(61.98), 'end': np.float64(62.16)}, {'word': 'ท', 'start': np.float64(62.5), 'end': np.float64(62.6)}, {'word': 'ี่', 'start': np.float64(62.6), 'end': np.float64(62.7)}, {'word': 'พ', 'start': np.float64(62.7), 'end': np.float64(62.9)}, {'word': 'ี', 'start': np.float64(62.9), 'end': np.float64(62.98)}, {'word': 'ก', 'start': np.float64(62.98), 'end': np.float64(63.12)}, {'word': 'ว', 'start': np.float64(63.12), 'end': np.float64(63.2)}, {'word': '่', 'start': np.float64(63.2), 'end': np.float64(63.2)}, {'word': 'า', 'start': np.float64(63.2), 'end': np.float64(63.28)}, {'word': 'เร', 'start': np.float64(63.28), 'end': np.float64(63.4)}, {'word': 'ื่', 'start': np.float64(63.4), 'end': np.float64(63.4)}, {'word': 'อง', 'start': np.float64(63.4), 'end': np.float64(63.46)}, {'word': 'พ', 'start': np.float64(63.46), 'end': np.float64(63.58)}, {'word': 'ี่', 'start': np.float64(63.58), 'end': np.float64(63.66)}, {'word': 'ส', 'start': np.float64(63.66), 'end': np.float64(63.78)}, {'word': 'า', 'start': np.float64(63.78), 'end': np.float64(63.86)}, {'word': 'ว', 'start': np.float64(63.86), 'end': np.float64(63.92)}, {'word': 'ฉ', 'start': np.float64(63.92), 'end': np.float64(64.0)}, {'word': 'ั', 'start': np.float64(64.0), 'end': np.float64(64.04)}, {'word': 'น', 'start': np.float64(64.04), 'end': np.float64(64.04)}, {'word': 'ไป', 'start': np.float64(64.04), 'end': np.float64(64.24)}, {'word': 'ข', 'start': np.float64(64.24), 'end': np.float64(64.36)}, {'word': 'ั', 'start': np.float64(64.36), 'end': np.float64(64.4)}, {'word': 'ด', 'start': np.float64(64.4), 'end': np.float64(64.46)}, {'word': 'ห', 'start': np.float64(64.46), 'end': np.float64(64.58)}, {'word': '้', 'start': np.float64(64.58), 'end': np.float64(64.64)}, {'word': 'อง', 'start': np.float64(64.64), 'end': np.float64(64.64)}, {'word': 'น', 'start': np.float64(64.64), 'end': np.float64(64.82)}, {'word': '้', 'start': np.float64(64.82), 'end': np.float64(64.88)}, {'word': 'ำ', 'start': np.float64(64.88), 'end': np.float64(64.98)}, {'word': 'ก', 'start': np.float64(64.98), 'end': np.float64(65.34)}, {'word': '็', 'start': np.float64(65.34), 'end': np.float64(65.4)}, {'word': 'ค', 'start': np.float64(65.4), 'end': np.float64(65.48)}, {'word': 'ื', 'start': np.float64(65.48), 'end': np.float64(65.48)}, {'word': 'อ', 'start': np.float64(65.48), 'end': np.float64(65.56)}, {'word': 'ค', 'start': np.float64(65.56), 'end': np.float64(65.66)}, {'word': 'ำ', 'start': np.float64(65.66), 'end': np.float64(65.76)}, {'word': 'ถ', 'start': np.float64(65.76), 'end': np.float64(65.88)}, {'word': 'าม', 'start': np.float64(65.88), 'end': np.float64(65.98)}, {'word': 'ท', 'start': np.float64(65.98), 'end': np.float64(66.1)}, {'word': 'ี่', 'start': np.float64(66.1), 'end': np.float64(66.14)}, {'word': 'ว', 'start': np.float64(66.14), 'end': np.float64(66.28)}, {'word': '่', 'start': np.float64(66.28), 'end': np.float64(66.34)}, {'word': 'า', 'start': np.float64(66.34), 'end': np.float64(66.48)}, {'word': 'แล', 'start': np.float64(66.48), 'end': np.float64(66.96)}, {'word': '้', 'start': np.float64(66.96), 'end': np.float64(67.04)}, {'word': 'ว', 'start': np.float64(67.04), 'end': np.float64(67.1)}, {'word': 'แ', 'start': np.float64(67.1), 'end': np.float64(67.3)}, {'word': 'ฟ', 'start': np.float64(67.3), 'end': np.float64(67.34)}, {'word': 'น', 'start': np.float64(67.34), 'end': np.float64(67.46)}, {'word': 'ฉ', 'start': np.float64(67.46), 'end': np.float64(67.68)}, {'word': 'ั', 'start': np.float64(67.68), 'end': np.float64(67.78)}, {'word': 'น', 'start': np.float64(67.78), 'end': np.float64(67.88)}, {'word': 'ไป', 'start': np.float64(67.98), 'end': np.float64(68.26)}, {'word': 'ทำ', 'start': np.float64(68.26), 'end': np.float64(68.54)}, {'word': 'อะไร', 'start': np.float64(68.54), 'end': np.float64(68.88)}, {'word': 'ท', 'start': np.float64(68.88), 'end': np.float64(69.04)}, {'word': 'ี่', 'start': np.float64(69.04), 'end': np.float64(69.14)}, {'word': 'น', 'start': np.float64(69.14), 'end': np.float64(69.26)}, {'word': 'ั่', 'start': np.float64(69.26), 'end': np.float64(69.34)}, {'word': 'น', 'start': np.float64(69.34), 'end': np.float64(69.4)}, {'word': 'ก', 'start': np.float64(69.4), 'end': np.float64(69.62)}, {'word': '่', 'start': np.float64(69.62), 'end': np.float64(69.68)}, {'word': 'อน', 'start': np.float64(69.68), 'end': np.float64(69.8)}, {'word': 'น', 'start': np.float64(69.8), 'end': np.float64(69.92)}, {'word': 'อน', 'start': np.float64(69.92), 'end': np.float64(70.02)}, {'word': 'น', 'start': np.float64(70.02), 'end': np.float64(70.22)}, {'word': 'อน', 'start': np.float64(70.22), 'end': np.float64(70.52)}]


    print("before alignment")

    aligned_word_data = align_transcription_to_script_and_correct_timestamps(
        original_script=test_perfect_thai_script,
        whisper_word_data = sample_raw_whisper_word_data
    )

    print("after alignment")
    # pprint(aligned_word_data, sort_dicts=False) # prints as a formatted json file